#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å›½åœŸæ•°å€¤æƒ…å ±ï¼ˆåœ°ä¾¡å…¬ç¤ºãƒ‡ãƒ¼ã‚¿ï¼‰å…¨26å¹´åˆ†ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Step 2: 2000-2025å¹´ã®å…¨26å¹´åˆ†ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
"""

import geopandas as gpd
import psycopg2
import yaml
from pathlib import Path
from dotenv import load_dotenv
import os
import sys
import re
from datetime import date
import pandas as pd

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


def load_db_config():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
    config_path = project_root / 'config' / 'database.yml'
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    return {
        'host': os.getenv('DB_HOST', config['postgresql'].get('host', 'localhost')),
        'port': int(os.getenv('DB_PORT', config['postgresql'].get('port', 5432))),
        'database': os.getenv('DB_NAME', config['postgresql'].get('database', 'real_estate_dev')),
        'user': os.getenv('DB_USER', config['postgresql'].get('user', 'postgres')),
        'password': os.getenv('DB_PASSWORD', config['postgresql'].get('password', 'postgres'))
    }


def normalize_address(address):
    """ä½æ‰€ã‚’æ­£è¦åŒ–"""
    if not address:
        return ""
    
    # å…¨è§’æ•°å­—ã‚’åŠè§’ã«
    address = address.translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789'))
    
    # ã‚¹ãƒšãƒ¼ã‚¹å‰Šé™¤
    address = address.replace(' ', '').replace('ã€€', '')
    
    # ã€Œæ±äº¬éƒ½ã€ã‚’å‰Šé™¤
    address = address.replace('æ±äº¬éƒ½', '')
    
    # ã€Œä¸–ç”°è°·åŒºã€ã‚’å‰Šé™¤
    if 'ä¸–ç”°è°·åŒº' in address:
        address = address.split('ä¸–ç”°è°·åŒº')[1]
    
    # å…¨è§’ãƒã‚¤ãƒ•ãƒ³ãƒ»ãƒã‚¤ãƒŠã‚¹ã‚’åŠè§’ãƒã‚¤ãƒ•ãƒ³ã«çµ±ä¸€
    address = address.replace('ï¼', '-').replace('âˆ’', '-').replace('â€', '-')
    
    # ãƒã‚¤ãƒ•ãƒ³å½¢å¼ã‚’ä¸ç›®ç•ªå½¢å¼ã«å¤‰æ›
    parts = re.split(r'[-]', address)
    if len(parts) == 3:
        # 3åˆ†å‰²ã•ã‚ŒãŸå ´åˆï¼šç”ºå-ä¸ç›®-ç•ªåœ°
        address = f"{parts[0]}ä¸ç›®{parts[1]}ç•ª{parts[2]}"
    elif len(parts) == 2:
        # 2åˆ†å‰²ã•ã‚ŒãŸå ´åˆï¼šç”ºå-ç•ªåœ°
        address = f"{parts[0]}ç•ª{parts[1]}"
    
    # ã€Œå¤–ã€ã‚’å‰Šé™¤ï¼ˆãƒãƒƒãƒãƒ³ã‚°ã®ãŸã‚ï¼‰
    address = address.replace('å¤–', '')
    
    return address.strip()


def create_table(conn):
    """ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆ"""
    cur = conn.cursor()
    
    create_table_sql = """
    CREATE TABLE IF NOT EXISTS land_prices_kokudo (
        id SERIAL PRIMARY KEY,
        choume_code VARCHAR(11),
        survey_year INTEGER NOT NULL,
        official_price INTEGER,
        data_source VARCHAR(50) NOT NULL DEFAULT 'åœ°ä¾¡å…¬ç¤º',
        original_address TEXT,
        land_area INTEGER,
        land_use VARCHAR(50),
        building_coverage_ratio INTEGER,
        floor_area_ratio INTEGER,
        road_direction VARCHAR(10),
        road_width NUMERIC(5,1),
        nearest_station VARCHAR(100),
        station_distance INTEGER,
        latitude NUMERIC(10,7),
        longitude NUMERIC(11,7),
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        UNIQUE(survey_year, original_address)
    );
    
    CREATE INDEX IF NOT EXISTS idx_land_prices_kokudo_year ON land_prices_kokudo(survey_year);
    CREATE INDEX IF NOT EXISTS idx_land_prices_kokudo_choume ON land_prices_kokudo(choume_code);
    """
    
    cur.execute(create_table_sql)
    conn.commit()
    cur.close()
    print("âœ… ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆå®Œäº†: land_prices_kokudo")


def get_field_mapping(pattern):
    """ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å–å¾—"""
    FIELD_MAPPINGS = {
        'pattern_A': {  # 2000-2011å¹´
            'city_code': 'L01_017',
            'city_name': 'L01_018',
            'address': 'L01_019',
            'price': 'L01_006',
            'land_area': 'L01_020',
        },
        'pattern_B': {  # 2012-2019å¹´ã€2022å¹´
            'city_code': 'L01_017',
            'city_name': 'L01_018',
            'address': 'L01_019',
            'price': 'L01_006',
            'land_area': 'L01_020',
        },
        'pattern_C': {  # 2018, 2020-2021, 2023å¹´
            'city_code': 'L01_021',
            'city_name': 'L01_022',
            'address': 'L01_023',
            'price': 'L01_006',
            'land_area': 'L01_024',
            'road_direction': 'L01_037',
            'road_width': 'L01_038',
            'nearest_station': 'L01_045',
            'station_distance': 'L01_046',
            'land_use': 'L01_047',
            'building_coverage': 'L01_052',
            'floor_area_ratio': 'L01_053',
        },
        'pattern_C_v2': {  # 2022-2023å¹´
            'city_code': 'L01_022',
            'city_name': 'L01_023',
            'address': 'L01_024',
            'price': 'L01_006',
            'land_area': 'L01_026',
            'road_direction': 'L01_040',
            'road_width': 'L01_041',
            'nearest_station': 'L01_048',
            'station_distance': 'L01_049',
            'land_use': 'L01_050',
            'building_coverage': 'L01_056',
            'floor_area_ratio': 'L01_057',
        },
        'pattern_C_v2_2024': {  # 2024-2025å¹´
            'city_code': 'L01_001',
            'city_name': 'L01_024',
            'address': 'L01_025',
            'price': 'L01_008',  # 2024-2025å¹´ã¯L01_008
            'land_area': 'L01_027',
            'road_direction': 'L01_041',
            'road_width': 'L01_042',
            'nearest_station': 'L01_048',
            'station_distance': 'L01_050',
            'land_use': 'L01_051',
            'building_coverage': 'L01_057',
            'floor_area_ratio': 'L01_058',
        }
    }
    return FIELD_MAPPINGS.get(pattern, {})


def extract_records(gdf, year, field_mapping):
    """GeoDataFrameã‹ã‚‰ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
    records = []
    
    for idx, row in gdf.iterrows():
        try:
            # ä½æ‰€ã‚’å–å¾—
            address_raw = row.get(field_mapping['address'], '')
            if pd.isna(address_raw) or address_raw == '':
                continue
            
            # ä½æ‰€ã‚’æ­£è¦åŒ–
            normalized_address = normalize_address(str(address_raw))
            
            # ä¾¡æ ¼ã‚’å–å¾—
            price = None
            if 'price' in field_mapping:
                price_raw = row.get(field_mapping['price'])
                # GeoJSONã¯å…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒæ–‡å­—åˆ—ãªã®ã§ã€ç„¡åŠ¹å€¤ã‚’ãƒã‚§ãƒƒã‚¯
                if pd.notna(price_raw) and str(price_raw) not in ['', '_', 'false', 'None']:
                    try:
                        price = int(float(price_raw))
                        # ä¾¡æ ¼ã®å˜ä½ç¢ºèª: ã‚‚ã—å°ã•ã™ãã‚‹å ´åˆã¯Ã—100
                        # ä¸–ç”°è°·åŒºã®å¹³å‡åœ°ä¾¡ã¯50-60ä¸‡å††/ã¡ç¨‹åº¦
                        if price < 10000:  # 1ä¸‡å††/ã¡æœªæº€ã¯ç•°å¸¸ï¼ˆå˜ä½ãŒ100å††å˜ä½ã®å¯èƒ½æ€§ï¼‰
                            price = price * 100
                    except (ValueError, TypeError):
                        price = None
            
            # åœ°ç©ã‚’å–å¾—
            land_area = None
            if 'land_area' in field_mapping:
                land_area_raw = row.get(field_mapping['land_area'])
                if pd.notna(land_area_raw) and str(land_area_raw) not in ['', '_', 'false']:
                    try:
                        land_area = int(float(land_area_raw))
                    except (ValueError, TypeError):
                        land_area = None
            
            # ã‚¸ã‚ªãƒ¡ãƒˆãƒªã‹ã‚‰åº§æ¨™ã‚’å–å¾—
            latitude = None
            longitude = None
            if hasattr(row, 'geometry') and row.geometry is not None:
                try:
                    if row.geometry.geom_type == 'Point':
                        longitude, latitude = row.geometry.x, row.geometry.y
                    else:
                        # Polygonç­‰ã®å ´åˆã¯é‡å¿ƒã‚’å–å¾—
                        centroid = row.geometry.centroid
                        longitude, latitude = centroid.x, centroid.y
                except:
                    pass
            
            record = {
                'survey_year': year,
                'original_address': normalized_address,
                'official_price': price,
                'land_area': land_area,
                'data_source': 'åœ°ä¾¡å…¬ç¤º',
                'latitude': latitude,
                'longitude': longitude,
                'created_at': date.today()
            }
            
            # è¿½åŠ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆ2018å¹´ä»¥é™ï¼‰
            if year >= 2018:
                if 'road_direction' in field_mapping:
                    val = row.get(field_mapping['road_direction'])
                    record['road_direction'] = str(val) if pd.notna(val) and val != '_' else None
                
                if 'road_width' in field_mapping:
                    val = row.get(field_mapping['road_width'])
                    if pd.notna(val) and val != '_' and val != 0:
                        try:
                            record['road_width'] = float(val)
                        except:
                            record['road_width'] = None
                    else:
                        record['road_width'] = None
                
                if 'nearest_station' in field_mapping:
                    val = row.get(field_mapping['nearest_station'])
                    record['nearest_station'] = str(val) if pd.notna(val) and val != '_' else None
                
                if 'station_distance' in field_mapping:
                    val = row.get(field_mapping['station_distance'])
                    if pd.notna(val) and val != '_' and val != 0:
                        try:
                            record['station_distance'] = int(float(val))
                        except:
                            record['station_distance'] = None
                    else:
                        record['station_distance'] = None
                
                if 'land_use' in field_mapping:
                    val = row.get(field_mapping['land_use'])
                    record['land_use'] = str(val) if pd.notna(val) and val != '_' else None
                
                if 'building_coverage' in field_mapping:
                    val = row.get(field_mapping['building_coverage'])
                    if pd.notna(val) and val != '_' and val != 0:
                        try:
                            record['building_coverage_ratio'] = int(float(val))
                        except:
                            record['building_coverage_ratio'] = None
                    else:
                        record['building_coverage_ratio'] = None
                
                if 'floor_area_ratio' in field_mapping:
                    val = row.get(field_mapping['floor_area_ratio'])
                    if pd.notna(val) and val != '_' and val != 0:
                        try:
                            record['floor_area_ratio'] = int(float(val))
                        except:
                            record['floor_area_ratio'] = None
                    else:
                        record['floor_area_ratio'] = None
            
            records.append(record)
            
        except Exception as e:
            print(f"    âš ï¸  è¡Œ {idx} ã®å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    return records


def import_to_db(conn, records):
    """PostgreSQLã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"""
    if not records:
        return 0, 0
    
    cur = conn.cursor()
    
    insert_sql = """
    INSERT INTO land_prices_kokudo (
        survey_year, original_address, official_price, land_area,
        data_source, latitude, longitude, created_at,
        land_use, building_coverage_ratio, floor_area_ratio,
        road_direction, road_width, nearest_station, station_distance
    ) VALUES (
        %(survey_year)s, %(original_address)s, %(official_price)s, %(land_area)s,
        %(data_source)s, %(latitude)s, %(longitude)s, %(created_at)s,
        %(land_use)s, %(building_coverage_ratio)s, %(floor_area_ratio)s,
        %(road_direction)s, %(road_width)s, %(nearest_station)s, %(station_distance)s
    )
    ON CONFLICT (survey_year, original_address) DO UPDATE SET
        official_price = EXCLUDED.official_price,
        land_area = EXCLUDED.land_area,
        land_use = EXCLUDED.land_use,
        building_coverage_ratio = EXCLUDED.building_coverage_ratio,
        floor_area_ratio = EXCLUDED.floor_area_ratio,
        road_direction = EXCLUDED.road_direction,
        road_width = EXCLUDED.road_width,
        nearest_station = EXCLUDED.nearest_station,
        station_distance = EXCLUDED.station_distance
    """
    
    success_count = 0
    error_count = 0
    
    for record in records:
        try:
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®š
            record.setdefault('land_use', None)
            record.setdefault('building_coverage_ratio', None)
            record.setdefault('floor_area_ratio', None)
            record.setdefault('road_direction', None)
            record.setdefault('road_width', None)
            record.setdefault('nearest_station', None)
            record.setdefault('station_distance', None)
            
            cur.execute(insert_sql, record)
            success_count += 1
        except Exception as e:
            print(f"    âŒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            print(f"       ãƒ‡ãƒ¼ã‚¿: {record.get('original_address', 'N/A')}")
            error_count += 1
            conn.rollback()
            continue
    
    conn.commit()
    cur.close()
    
    return success_count, error_count


def get_file_config():
    """å…¨26å¹´åˆ†ã®ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®šã‚’å–å¾—"""
    base_path = project_root / "data" / "raw" / "national" / "kokudo_suuchi"
    
    config = {}
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³A: 2000-2011å¹´ï¼ˆ12å¹´ï¼‰
    for year in range(2000, 2012):
        year_short = f"{year % 100:02d}"
        config[year] = {
            'filepath': base_path / f"{year}_13" / f"L01-{year_short}_13-g_LandPrice.shp",
            'format': 'shapefile',
            'field_mapping': 'pattern_A'
        }
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³B: 2012-2014å¹´ï¼ˆ3å¹´ï¼‰
    for year in range(2012, 2015):
        year_short = f"{year % 100:02d}"
        config[year] = {
            'filepath': base_path / f"{year}_13" / f"L01-{year_short}_13.shp",
            'format': 'shapefile',
            'field_mapping': 'pattern_B'
        }
    
    # 2015å¹´ï¼ˆGMLã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®Shapefileï¼‰
    config[2015] = {
        'filepath': base_path / "2015_13" / "L01-15_13_GML" / "L01-15_13.shp",
        'format': 'shapefile',
        'field_mapping': 'pattern_B'
    }
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³Bç¶šã: 2016-2017å¹´ï¼ˆ2å¹´ï¼‰
    for year in range(2016, 2018):
        year_short = f"{year % 100:02d}"
        config[year] = {
            'filepath': base_path / f"{year}_13" / f"L01-{year_short}_13.shp",
            'format': 'shapefile',
            'field_mapping': 'pattern_B'
        }
    
    # 2018å¹´ï¼ˆGMLã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®GeoJSONï¼‰
    config[2018] = {
        'filepath': base_path / "2018_13" / "L01-18_13_GML" / "L01-18_13.geojson",
        'format': 'geojson',
        'field_mapping': 'pattern_C'
    }
    
    # 2019å¹´ï¼ˆGeoJSONã‚’ä½¿ç”¨ã€Shapefileã¯å±æ€§ãƒ‡ãƒ¼ã‚¿ãªã—ï¼‰
    config[2019] = {
        'filepath': base_path / "2019_13" / "L01-19_13.geojson",
        'format': 'geojson',
        'field_mapping': 'pattern_C'
    }
    
    # 2020-2021å¹´ï¼ˆGMLã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®GeoJSONï¼‰
    for year in range(2020, 2022):
        year_short = f"{year % 100:02d}"
        config[year] = {
            'filepath': base_path / f"{year}_13" / f"L01-{year_short}_13_GML" / f"L01-{year_short}_13.geojson",
            'format': 'geojson',
            'field_mapping': 'pattern_C'
        }
    
    # 2022å¹´ï¼ˆGeoJSONã‚’ä½¿ç”¨ã€Shapefileã¯å±æ€§ãƒ‡ãƒ¼ã‚¿ãªã—ï¼‰
    config[2022] = {
        'filepath': base_path / "2022_13" / "L01-22_13.geojson",
        'format': 'geojson',
        'field_mapping': 'pattern_C_v2'
    }
    
    # 2023å¹´ï¼ˆGMLã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®GeoJSONã€ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°ä¿®æ­£ï¼‰
    config[2023] = {
        'filepath': base_path / "2023_13" / "L01-23_13_GML" / "L01-23_13.geojson",
        'format': 'geojson',
        'field_mapping': 'pattern_C_v2'
    }
    
    # 2024-2025å¹´ï¼ˆGMLã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®GeoJSONã€ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°ãŒç•°ãªã‚‹ï¼‰
    for year in range(2024, 2026):
        year_short = f"{year % 100:02d}"
        config[year] = {
            'filepath': base_path / f"{year}_13" / f"L01-{year_short}_13_GML" / f"L01-{year_short}_13.geojson",
            'format': 'geojson',
            'field_mapping': 'pattern_C_v2_2024'
        }
    
    return config


def main():
    print("=" * 80)
    print("å›½åœŸæ•°å€¤æƒ…å ±ï¼ˆåœ°ä¾¡å…¬ç¤ºãƒ‡ãƒ¼ã‚¿ï¼‰å…¨26å¹´åˆ†ã‚¤ãƒ³ãƒãƒ¼ãƒˆ")
    print("Step 2: 2000-2025å¹´ã®å…¨26å¹´åˆ†")
    print("=" * 80)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
    db_config = load_db_config()
    try:
        conn = psycopg2.connect(**db_config)
        print("\nâœ… PostgreSQLã«æ¥ç¶šã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ PostgreSQLæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
    create_table(conn)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®šå–å¾—
    file_config = get_file_config()
    
    total_imported = 0
    success_years = []
    failed_years = []
    
    # å…¨26å¹´åˆ†ã‚’å‡¦ç†
    for year in range(2000, 2026):
        print(f"\n{'='*80}")
        print(f"ğŸ“‚ {year}å¹´ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆä¸­...")
        print('='*80)
        
        try:
            config = file_config[year]
            filepath = config['filepath']
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
            if not filepath.exists():
                print(f"  âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filepath}")
                failed_years.append(year)
                continue
            
            # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            print(f"  ğŸ“‚ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {filepath.name}")
            if config['format'] == 'shapefile':
                gdf = gpd.read_file(filepath, encoding='shift-jis')
            else:
                gdf = gpd.read_file(filepath, encoding='utf-8')
            
            print(f"    ç·ä»¶æ•°: {len(gdf):,}ä»¶")
            
            # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°å–å¾—
            field_mapping = get_field_mapping(config['field_mapping'])
            
            # ä¸–ç”°è°·åŒºãƒ•ã‚£ãƒ«ã‚¿
            city_code_field = field_mapping['city_code']
            setagaya = gdf[gdf[city_code_field].astype(str) == '13112']
            print(f"    ä¸–ç”°è°·åŒº: {len(setagaya):,}ä»¶")
            
            if len(setagaya) == 0:
                print(f"  âš ï¸  ä¸–ç”°è°·åŒºã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                failed_years.append(year)
                continue
            
            # ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            records = extract_records(setagaya, year, field_mapping)
            print(f"  âœ… ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºå®Œäº†: {len(records)}ä»¶")
            
            # ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            success, errors = import_to_db(conn, records)
            print(f"  âœ… ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†: æˆåŠŸ {success}ä»¶ã€ã‚¨ãƒ©ãƒ¼ {errors}ä»¶")
            
            total_imported += success
            success_years.append(year)
            
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            failed_years.append(year)
            continue
    
    conn.close()
    
    # çµæœã‚µãƒãƒªãƒ¼
    print(f"\n{'='*80}")
    print(f"âœ… å®Œäº†")
    print('='*80)
    print(f"  æˆåŠŸ: {len(success_years)}å¹´åˆ† / {total_imported:,}ä»¶")
    print(f"  å¤±æ•—: {len(failed_years)}å¹´åˆ†")
    
    if failed_years:
        print(f"\n  å¤±æ•—ã—ãŸå¹´åº¦: {', '.join(map(str, failed_years))}")
    
    if success_years:
        print(f"\n  æˆåŠŸã—ãŸå¹´åº¦: {', '.join(map(str, success_years))}")
    
    print("=" * 80)


if __name__ == '__main__':
    main()

