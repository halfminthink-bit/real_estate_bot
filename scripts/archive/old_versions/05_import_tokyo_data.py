import pandas as pd
import psycopg2
from pathlib import Path
from loguru import logger
from datetime import date, datetime
import re

# DBæ¥ç¶šè¨­å®š
DB_CONFIG = {
    'host': 'localhost',
    'port': 5432,
    'database': 'real_estate_dev',
    'user': 'postgres',
    'password': 'postgres'
}

def clean_price(price_str):
    """
    ä¾¡æ ¼æ–‡å­—åˆ—ã‚’intã«å¤‰æ›
    ä¾‹: '1,234,567' -> 1234567
    """
    if pd.isna(price_str):
        return None
    # ã‚«ãƒ³ãƒã‚’é™¤å»ã—ã¦æ•°å€¤ã«å¤‰æ›
    price_str = str(price_str).replace(',', '').replace('å††', '').strip()
    try:
        return int(float(price_str))
    except:
        return None

def convert_wareki_to_year(wareki):
    """
    å’Œæš¦ã‚’è¥¿æš¦ã«å¤‰æ›
    ä¾‹: 7 (ä»¤å’Œ7å¹´) -> 2025
    """
    if pd.isna(wareki):
        return None
    wareki = int(wareki)
    # ä»¤å’Œã¯2019å¹´ã‹ã‚‰
    return 2018 + wareki

def load_tokyo_csv(csv_path: Path, year: int):
    """
    æ±äº¬éƒ½ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ã®CSVã‚’èª­ã¿è¾¼ã‚€
    
    Args:
        csv_path: CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        year: å¯¾è±¡å¹´ï¼ˆè¥¿æš¦ï¼‰
    """
    logger.info(f'ğŸ“‚ èª­ã¿è¾¼ã¿: {csv_path}')
    
    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦ã™
    encodings = ['cp932', 'shift-jis', 'utf-8']
    df = None
    
    # 2021-2023å¹´ã¯ã‚¿ã‚¤ãƒˆãƒ«è¡Œãªã—ã€ãã‚Œä»¥å¤–ã¯ã‚¿ã‚¤ãƒˆãƒ«è¡Œã‚ã‚Š
    skiprows = 0 if 2021 <= year <= 2023 else 1
    
    for enc in encodings:
        try:
            df = pd.read_csv(csv_path, encoding=enc, skiprows=skiprows)
            logger.info(f'âœ… èª­ã¿è¾¼ã¿æˆåŠŸï¼ˆ{enc}ï¼‰: {len(df)} ä»¶')
            logger.info(f'   å¹´åº¦: {year}, skiprows: {skiprows}')
            break
        except Exception as e:
            continue
    
    if df is None:
        logger.error('âŒ CSVã®èª­ã¿è¾¼ã¿ã«å¤±æ•—')
        return None
    
    logger.info(f'ã‚«ãƒ©ãƒ : {df.columns.tolist()[:10]}...')
    
    return df

def parse_tokyo_data(df: pd.DataFrame, year: int):
    """
    æ±äº¬éƒ½CSVã‚’çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›
    """
    records = []
    
    # ã‚«ãƒ©ãƒ åã®ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆå¹´åº¦ã«ã‚ˆã£ã¦å¾®å¦™ã«é•ã†ï¼‰
    col_mapping = {
        'éƒ½é“åºœçœŒå¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰': 'city_code',
        'åŒºå¸‚ç”ºæ‘å': 'city_name',
        'åœ°ç•ª': 'address',
        'ä½å±…è¡¨ç¤º': 'residential_address',
        'å½“å¹´ä¾¡æ ¼ï¼ˆå††ï¼‰': 'current_price',
        'å½“å¹´ä¾¡æ ¼': 'current_price',
        'å¯¾å‰å¹´å¤‰å‹•ç‡ï¼ˆï¼…ï¼‰': 'yoy_change',
        'å¯¾å‰å¹´å¤‰å‹•ç‡ï¼ˆï¼…ï¼‰': 'yoy_change',
        'ç”¨é€”åŒºåˆ†': 'land_type',
        'æ³•è¦åˆ¶ãƒ»ç”¨é€”åŒºåˆ†': 'land_type',
    }
    
    # ä¸–ç”°è°·åŒºã®ã¿æŠ½å‡ºï¼ˆ13112ï¼‰
    if 'éƒ½é“åºœçœŒå¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰' in df.columns:
        df = df[df['éƒ½é“åºœçœŒå¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰'] == 13112].copy()
    elif 'æ¨™æº–åœ°ç•ªå·ï¼ˆéƒ½é“åºœçœŒå¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰ï¼‰' in df.columns:
        df = df[df['æ¨™æº–åœ°ç•ªå·ï¼ˆéƒ½é“åºœçœŒå¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰ï¼‰'] == 13112].copy()
    
    logger.info(f'ä¸–ç”°è°·åŒºã®ãƒ‡ãƒ¼ã‚¿: {len(df)} ä»¶')
    
    for idx, row in df.iterrows():
        try:
            # ä¾¡æ ¼
            if 'å½“å¹´ä¾¡æ ¼ï¼ˆå††ï¼‰' in df.columns:
                price = clean_price(row['å½“å¹´ä¾¡æ ¼ï¼ˆå††ï¼‰'])
            elif 'å½“å¹´ä¾¡æ ¼' in df.columns:
                price = clean_price(row['å½“å¹´ä¾¡æ ¼'])
            else:
                logger.warning(f'ä¾¡æ ¼ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“')
                continue
            
            if price is None or price == 0:
                continue
            
            # å¤‰å‹•ç‡
            if 'å¯¾å‰å¹´å¤‰å‹•ç‡ï¼ˆï¼…ï¼‰' in df.columns:
                yoy_change = row['å¯¾å‰å¹´å¤‰å‹•ç‡ï¼ˆï¼…ï¼‰']
            elif 'å¯¾å‰å¹´å¤‰å‹•ç‡' in df.columns:
                yoy_change = row['å¯¾å‰å¹´å¤‰å‹•ç‡']
            else:
                yoy_change = None
            
            # ä½æ‰€
            if 'åœ°ç•ª' in df.columns:
                address = row['åœ°ç•ª']
            elif 'æ‰€åœ¨ä¸¦ã³ã«åœ°ç•ª' in df.columns:
                address = row['æ‰€åœ¨ä¸¦ã³ã«åœ°ç•ª']
            else:
                address = ''
            
            # ç”¨é€”
            if 'ç”¨é€”åŒºåˆ†' in df.columns:
                land_type_raw = row['ç”¨é€”åŒºåˆ†']
            elif 'æ³•è¦åˆ¶ãƒ»ç”¨é€”åŒºåˆ†' in df.columns:
                land_type_raw = row['æ³•è¦åˆ¶ãƒ»ç”¨é€”åŒºåˆ†']
            else:
                land_type_raw = ''
            
            # ç”¨é€”åŒºåˆ†ã‚’æ¨™æº–åŒ–
            land_type = 'ä¸æ˜'
            if pd.notna(land_type_raw):
                land_type_str = str(land_type_raw)
                if 'ä½å®…' in land_type_str or 'ä½å±¤' in land_type_str:
                    land_type = 'ä½å®…åœ°'
                elif 'å•†æ¥­' in land_type_str:
                    land_type = 'å•†æ¥­åœ°'
                elif 'å·¥æ¥­' in land_type_str:
                    land_type = 'å·¥æ¥­åœ°'
            
            record = {
                'choume_code': 'UNKNOWN',  # å¾Œã§ä½æ‰€ã‹ã‚‰æŠ½å‡º
                'survey_year': year,
                'land_type': land_type,
                'official_price': price,
                'year_on_year_change': yoy_change if pd.notna(yoy_change) else None,
                'data_source': 'tokyo_opendata',
                'original_address': str(address) if pd.notna(address) else '',
                'latitude': None,
                'longitude': None,
                'created_at': date.today()
            }
            
            records.append(record)
            
        except Exception as e:
            logger.warning(f'è¡Œ {idx} ã®å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}')
            continue
    
    logger.info(f'âœ… å¤‰æ›å®Œäº†: {len(records)} ä»¶')
    return records

def insert_to_db(records: list):
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    """
    if not records:
        logger.warning('ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“')
        return
    
    conn = psycopg2.connect(**DB_CONFIG)
    cursor = conn.cursor()
    
    insert_query = """
        INSERT INTO land_prices (
            choume_code, survey_year, land_type, official_price,
            year_on_year_change, data_source, original_address,
            latitude, longitude, created_at
        ) VALUES (
            %(choume_code)s, %(survey_year)s, %(land_type)s, %(official_price)s,
            %(year_on_year_change)s, %(data_source)s, %(original_address)s,
            %(latitude)s, %(longitude)s, %(created_at)s
        )
        ON CONFLICT (choume_code, survey_year, land_type, data_source, original_address)
        DO UPDATE SET
            official_price = EXCLUDED.official_price,
            year_on_year_change = EXCLUDED.year_on_year_change,
            latitude = EXCLUDED.latitude,
            longitude = EXCLUDED.longitude
    """
    
    success_count = 0
    error_count = 0
    
    for record in records:
        try:
            cursor.execute(insert_query, record)
            success_count += 1
        except Exception as e:
            logger.error(f'ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}')
            logger.error(f'ãƒ‡ãƒ¼ã‚¿: {record}')
            error_count += 1
            conn.rollback()
            continue
    
    conn.commit()
    cursor.close()
    conn.close()
    
    logger.info(f'âœ… ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†: æˆåŠŸ {success_count} ä»¶ã€ã‚¨ãƒ©ãƒ¼ {error_count} ä»¶')

def main():
    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®è¨­å®š
    log_dir = Path('logs')
    log_dir.mkdir(exist_ok=True)
    log_file = log_dir / f'import_tokyo_data_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
    
    # æ—¢å­˜ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’å‰Šé™¤ã—ã¦ã‹ã‚‰æ–°ã—ã„ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¿½åŠ 
    logger.remove()
    logger.add(
        log_file,
        format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}",
        level="INFO",
        encoding="utf-8"
    )
    logger.add(
        lambda msg: print(msg, end=""),  # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«ã‚‚å‡ºåŠ›
        format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}",
        level="INFO"
    )
    
    logger.info('=' * 60)
    logger.info('æ±äº¬éƒ½ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ ã‚¤ãƒ³ãƒãƒ¼ãƒˆ')
    logger.info(f'ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}')
    logger.info('=' * 60)
    
    # 8å¹´åˆ†ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    years = [
        (2025, 'tokyo_land_price_2025.csv'),
        (2024, 'tokyo_land_price_2024.csv'),
        (2023, 'tokyo_land_price_2023.csv'),
        (2022, 'tokyo_land_price_2022.csv'),
        (2021, 'tokyo_land_price_2021.csv'),
        (2020, 'tokyo_land_price_2020.csv'),
        (2019, 'tokyo_land_price_2019.csv'),
        (2018, 'tokyo_land_price_2018.csv'),
    ]
    
    base_dir = Path('data/raw/prefecture/tokyo')
    
    for year, filename in years:
        logger.info(f'\n--- {year}å¹´ ---')
        csv_path = base_dir / filename
        
        if not csv_path.exists():
            logger.warning(f'âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}')
            continue
        
        # CSVã‚’èª­ã¿è¾¼ã¿
        df = load_tokyo_csv(csv_path, year)
        if df is None:
            continue
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‘ãƒ¼ã‚¹
        records = parse_tokyo_data(df, year)
        
        # DBã«æŠ•å…¥
        insert_to_db(records)
    
    logger.info('\n' + '=' * 60)
    logger.info('ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ')
    logger.info('=' * 60)

if __name__ == '__main__':
    main()